# Ollama Model Recommendation for MCP/Tool Calling - Freya v2.0

**Date**: December 3, 2025  
**Author**: Manus AI  
**Version**: 1.0  
**Status**: ‚úÖ **Recommendation Complete**

---

## 1. Executive Summary

**Top Recommendation**: **Llama 3.1 8B-Instruct**  
**Fallback/Upgrade**: **Qwen 2.5 32B**

After comprehensive research into Ollama models with tool calling capabilities, **Llama 3.1 8B-Instruct** emerges as the best overall choice for Freya v2.0. It offers an excellent balance of performance, resource usage, and reliability, making it ideal for the initial MCP Gateway implementation and beyond.

**Key Findings**:
- **Model size matters**: 8B+ models are significantly more reliable for tool calling than 7B and smaller models.
- **Qwen 2.5 32B** is the most reliable but resource-intensive.
- **Llama 3.1 8B** is the best balance of performance and efficiency.
- **Granite 3.2 8B** is a strong emerging alternative.
- **Configuration is key**: Context length, temperature, and quantization significantly impact tool calling performance.

---

## 2. Research Sources

This recommendation is based on a combination of:

1.  **Official Ollama Documentation**: Tool calling capabilities and examples [1].
2.  **Expert Technical Articles**: "Best Ollama Models for Function Calling" guide [2].
3.  **Community Feedback**: Reddit discussions on r/n8n and r/ollama [3].
4.  **Hardware Constraints**: RTX 5060 Ti 16GB VRAM analysis.

---

## 3. Top 5 Models for MCP/Tool Calling

| Rank | Model | Rating | Key Strengths | Best For |
|:----:|:------|:------:|:--------------|:---------|
| **1** | **Llama 3.1 8B-Instruct** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Balanced performance, reliability, efficiency | **Freya Phase 1.5** |
| **2** | **Qwen 2.5 32B** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Maximum reliability, complex multi-tool | **Freya Fallback** |
| **3** | **Granite 3.2 8B** | ‚≠ê‚≠ê‚≠ê‚≠ê | Strong alternative to Llama 3.1 8B | Testing & comparison |
| **4** | **Qwen 2.5 Coder 14B** | ‚≠ê‚≠ê‚≠ê‚≠ê | Developer tools, API integration | Code-related tasks |
| **5** | **Mistral 7B-Instruct v0.3** | ‚≠ê‚≠ê‚≠ê‚≠ê | Low latency, multilingual | Resource-constrained envs |

---

## 4. Detailed Model Comparison

### ü•á Llama 3.1 8B-Instruct (Top Recommendation)

**Why it wins**:
- **Best Balance**: Excellent tool calling performance without excessive hardware requirements.
- **Reliable**: Official Meta model, widely tested and supported.
- **Efficient**: Fits easily on RTX 5060 Ti 16GB, even with higher precision quantization.
- **Fast**: Good inference speed for real-time conversation.
- **Community Trust**: Used in official Ollama examples and recommended by experts.

**Quote from Collabnix**:
> "Llama 3.1 8B-Instruct stands out as the **best overall choice** for function calling applications." [2]

### ü•à Qwen 2.5 32B (The Powerhouse)

**Why it's the fallback**:
- **Most Reliable**: Handles complex multi-tool scenarios with the highest accuracy.
- **Handles Complexity**: Best for scenarios with 10+ tools or sequential tool calls.
- **Proven Performer**: Community consensus on its reliability.

**Quote from Reddit**:
> "The most reliable model I've found for this is qwen2.5:32b" [3]

**Drawbacks**:
- **Resource Intensive**: Requires ~20GB VRAM with Q4 quantization, which is a tight fit on the RTX 5060 Ti 16GB.
- **Slower**: Inference speed is noticeably slower than 8B models.

### ü•â Granite 3.2 8B (The Rising Star)

**Why it's worth watching**:
- **Strong Performer**: Emerging as a strong competitor to Llama 3.1 8B.
- **IBM Backed**: Developed by IBM, offering a different flavor of model.
- **Efficient**: Similar resource usage to Llama 3.1 8B.

**Community Feedback**:
> "I'm also quite impressed with the new granite3.2:8b, but have more testing to do there" [3]

---

## 5. Hardware Fit Analysis (RTX 5060 Ti 16GB)

| Model | Quantization | VRAM Usage | Fits? | Speed | Recommendation |
|:------|:-------------|:-----------|:------|:------|:-----------------|
| **Llama 3.1 8B** | Q4 | ~5GB | ‚úÖ Yes | Fast | Good |
| **Llama 3.1 8B** | **Q6** | **~7GB** | ‚úÖ **Yes** | **Fast** | **üèÜ BEST BALANCE** |
| **Llama 3.1 8B** | Q8 | ~9GB | ‚úÖ Yes | Medium | Good |
| **Qwen 2.5 32B** | **Q4** | **~20GB** | ‚ö†Ô∏è **Tight** | **Slow** | **üöÄ UPGRADE OPTION** |
| **Qwen 2.5 32B** | Q6 | ~32GB | ‚ùå No | Slow | N/A |
| **Granite 3.2 8B** | Q4 | ~5GB | ‚úÖ Yes | Fast | Good |
| **Granite 3.2 8B** | Q6 | ~7GB | ‚úÖ Yes | Fast | Good |

**Key Insight**: The RTX 5060 Ti 16GB can comfortably run the recommended **Llama 3.1 8B** with higher precision (Q6), which is ideal for tool calling. It can also run the more powerful **Qwen 2.5 32B** with Q4 quantization, but it will be slower and leave less room for other processes.

---

## 6. Best Practices for Tool Calling

To get the best results from any model, we must implement these best practices:

### 1. **Model Configuration**
- **Context Length**: Increase to 16K (`--context-length 16384`)
- **Temperature**: Lower to 0.1 for deterministic tool calls
- **Quantization**: Use Q6 or Q8 for better precision

### 2. **Prompting Strategy**
- **Be Explicit**: Tell the model it has tools and is in an agent loop
- **Lead the Model**: Break down complex tasks into smaller steps
- **Provide Examples**: Use few-shot prompting for complex tools

### 3. **Debugging**
- **Enable Ollama Debug Logging**: `OLLAMA_DEBUG=1` to see the exact prompt

### 4. **Tool Schema Design**
- **Clear Names**: Use descriptive, unambiguous function names
- **Detailed Descriptions**: Explain what each parameter does
- **JSON Schema**: Use strict JSON schema validation

---

## 7. Recommended Configuration for Freya v2.0

Based on this research, I recommend the following configuration in `config/default.yaml`:

```yaml
ollama:
  host: "http://localhost:11434"
  models:
    # Primary model for conversation and tool calling
    main: "llama3.1:8b-instruct-q6_K"
    
    # Fallback for complex scenarios (if needed)
    complex: "qwen2.5:32b-q4_K_M"
    
    # Alternative to test
    alternative: "granite3.2:8b-q6_K"
    
  # Tool calling settings
  tool_calling:
    temperature: 0.1  # Low for deterministic function calls
    context_length: 16384  # 16K context
    
  # Model selection strategy
  strategy: "adaptive"  # Use main model, fall back to complex if needed
```

**Rationale for this configuration**:
- **`main`**: `llama3.1:8b-instruct-q6_K` is the best balance of performance, precision, and resource usage.
- **`complex`**: `qwen2.5:32b-q4_K_M` is the most reliable model, used as a fallback for complex multi-tool tasks.
- **`alternative`**: `granite3.2:8b-q6_K` is available for testing and comparison.
- **`tool_calling`**: Settings are optimized for reliable tool use.
- **`strategy`**: An adaptive strategy allows Freya to use the best model for the job.

---

## 8. Implementation Plan

### 1. **Model Download**

```bash
# Pull the recommended models
ollama pull llama3.1:8b-instruct-q6_K
ollama pull qwen2.5:32b-q4_K_M
ollama pull granite3.2:8b-q6_K
```

### 2. **LLM Engine Update**

Update `src/services/llm/llm_engine.py` to:
- Load the `main` model by default
- Use the `tool_calling` settings from the config
- Implement the `adaptive` model selection strategy (future enhancement)

### 3. **Testing Strategy**

During Phase 1.5 (MCP Gateway), we will:
1.  Start with `llama3.1:8b-instruct-q6_K`
2.  Test with all 7 essential MCP tools
3.  Verify accuracy of tool selection and parameter extraction
4.  If issues arise, test with `qwen2.5:32b-q4_K_M`
5.  Compare performance and reliability

---

## 9. Conclusion

**Llama 3.1 8B-Instruct** is the clear winner for Freya v2.0's initial MCP implementation. It provides the best balance of performance, reliability, and resource efficiency for your hardware. By starting with this model and having a more powerful fallback (Qwen 2.5 32B) available, we can ensure a robust and capable tool calling system from day one.

This recommendation aligns with your preference for **quality and accuracy over speed**, as we are selecting a model known for reliable tool calling and using a higher-precision quantization (Q6).

---

## 10. References

[1] Ollama. (2025). *Tool calling*. Ollama Documentation. [https://docs.ollama.com/capabilities/tool-calling](https://docs.ollama.com/capabilities/tool-calling)

[2] Collabnix. (2025). *Best Ollama Models for Function Calling Tools: Complete Guide 2025*. Collabnix. [https://collabnix.com/best-ollama-models-for-function-calling-tools-complete-guide-2025/](https://collabnix.com/best-ollama-models-for-function-calling-tools-complete-guide-2025/)

[3] Reddit. (2025). *Best Ollama Models for Function Calling & Tools?*. r/n8n. [https://www.reddit.com/r/n8n/comments/1j25ten/best_ollama_models_for_function_calling_tools/](https://www.reddit.com/r/n8n/comments/1j25ten/best_ollama_models_for_function_calling_tools/)
